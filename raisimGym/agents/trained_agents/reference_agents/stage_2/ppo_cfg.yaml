seed: 1
record_video: yes

environment:
  simulation_dt: 0.0025
  control_dt: 0.01
  max_time: 15.0
  render: True

  num_envs: 240
  vis_every_n: 50
  visualize_eval: True
  num_threads: 40

  reward:
    position:
      coeff: -0.003
    thrust:
      coeff: 0.0
    orientation:
      coeff: -0.002
    angularVelocity:
      coeff: -0.0005


architecture:
  shared_nets: False # if True: actor and critic share the base net.
  base_net: [96]
  policy_net: [64, 64] # if shared_nets: True: first element in list will be skipped. i.e. actor_net will
  value_net: [64, 64]  # be set with [base_net, policy_net[1]]. For more layers, modify actor_critic.py.
  activation_fn: nn.Tanh
  deterministic_policy: False # if True: does not converge, bc. action loss will be used instead of log prob action loss
                              # False: loss = -logProb

helper:
  normalize_ob: True # (obs - obs_rms.mean) / obs_rms.var
  update_mean: False # update mean and var ob obs_rms
  scale_action: False # because scaling is done in Environment.hpp for better performance
  clip_action: False # either scale or clip action to [-1, 1]. Also works fine.

hyperparam:
  num_mini_batches: 3
  num_learning_epochs: 6
  shuffle: False
  Gamma: 0.998
  Lambda: 0.95
  clip_param: 0.2
  learning_rate: 0.00075
  value_loss_coef: 0.5
  use_clipped_value_loss: True # if True: PPO2, False: PPO
  l2_reg_coef: 0.0001
  bc_coef: 0.005
  entropy_coef: 0.0001

