seed: 1
record_video: yes

environment:
  simulation_dt: 0.0025
  control_dt: 0.01
  max_time: 15.0
  render: True

  num_envs: 240
  vis_every_n: 20
  visualize_eval: True
  num_threads: 40

  reward:
    position:
      coeff: -0.001
    thrust:
      coeff: 0.0
    orientation:
      coeff: -0.002
    angularVelocity:
      coeff: -0.0002


architecture:
  shared_nets: False # if True: actor and critic share the base net.
  base_net: [96]
  policy_net: [64, 64] # if shared_nets: True: first element in list will be skipped. i.e. actor_net will
  value_net: [64, 64]  # be set with [base_net, policy_net[1]]. For more layers, modify actor_critic.py.
  activation_fn: nn.Tanh
  deterministic_policy: False # if True: does not converge, bc. action loss will be used instead of log prob action loss
                              # False: loss = -logProb

helper:
  normalize_ob: True # True: (obs - obs_rms.mean) / obs_rms.var
  update_mean: False # True: update mean and var ob obs_rms
  scale_action: False # False: scaling is done in Environment.hpp for better performance
  clip_action: False # True: instead of scaling, action is clipped to [-1, 1].


hyperparam:
  num_mini_batches: 6
  num_learning_epochs: 8
  shuffle: False # True might give better sample efficiency
  gamma: 0.998
  lam: 0.95
  init_beta: 0.5
  beta_min: 0.3
  beta_scheduler: 0.001
  use_lr_scheduler: False # using torch.optim.lr_scheduler.CyclicLR
  max_lr: 0.002
  min_lr: 0.001 # is the actual learning rate if use_lr_scheduler = False
  l2_reg_weight: 0.0001
  entropy_weight: 0.0001

